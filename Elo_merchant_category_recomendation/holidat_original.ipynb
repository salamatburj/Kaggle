{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import date, datetime\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "\n",
    "import workalendar\n",
    "from workalendar.america import Brazil\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(4590)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  4.04 Mb (56.2% reduction)\n",
      "Mem. usage decreased to  2.24 Mb (52.5% reduction)\n",
      "Mem. usage decreased to 1749.11 Mb (43.7% reduction)\n",
      "Mem. usage decreased to 114.20 Mb (45.5% reduction)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('C:/Users/user/Documents/Salamat/ELO/train.csv')\n",
    "df_test = pd.read_csv('C:/Users/user/Documents/Salamat/ELO/test.csv')\n",
    "df_hist_trans = pd.read_csv('C:/Users/user/Documents/Salamat/ELO/historical_transactions.csv')\n",
    "df_new_merchant_trans = pd.read_csv('C:/Users/user/Documents/Salamat/ELO/new_merchant_transactions.csv')\n",
    "df_train=reduce_mem_usage(df_train)\n",
    "df_test=reduce_mem_usage(df_test)\n",
    "df_hist_trans=reduce_mem_usage(df_hist_trans)\n",
    "df_new_merchant_trans=reduce_mem_usage(df_new_merchant_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_hist_trans,df_new_merchant_trans]:\n",
    "   df['category_2'].fillna(1.0,inplace=True)\n",
    "   df['category_3'].fillna('A',inplace=True)\n",
    "   df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_columns(name,aggs):\n",
    "    return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011 [(datetime.date(2011, 1, 1), 'New year'), (datetime.date(2011, 4, 21), \"Tiradentes' Day\"), (datetime.date(2011, 5, 1), 'Labour Day'), (datetime.date(2011, 9, 7), 'Independence Day'), (datetime.date(2011, 10, 12), 'Our Lady of Aparecida'), (datetime.date(2011, 11, 2), \"All Souls' Day\"), (datetime.date(2011, 11, 15), 'Republic Day'), (datetime.date(2011, 12, 25), 'Christmas Day')]\n",
      "2012 [(datetime.date(2012, 1, 1), 'New year'), (datetime.date(2012, 4, 21), \"Tiradentes' Day\"), (datetime.date(2012, 5, 1), 'Labour Day'), (datetime.date(2012, 9, 7), 'Independence Day'), (datetime.date(2012, 10, 12), 'Our Lady of Aparecida'), (datetime.date(2012, 11, 2), \"All Souls' Day\"), (datetime.date(2012, 11, 15), 'Republic Day'), (datetime.date(2012, 12, 25), 'Christmas Day')]\n",
      "2013 [(datetime.date(2013, 1, 1), 'New year'), (datetime.date(2013, 4, 21), \"Tiradentes' Day\"), (datetime.date(2013, 5, 1), 'Labour Day'), (datetime.date(2013, 9, 7), 'Independence Day'), (datetime.date(2013, 10, 12), 'Our Lady of Aparecida'), (datetime.date(2013, 11, 2), \"All Souls' Day\"), (datetime.date(2013, 11, 15), 'Republic Day'), (datetime.date(2013, 12, 25), 'Christmas Day')]\n",
      "2014 [(datetime.date(2014, 1, 1), 'New year'), (datetime.date(2014, 4, 21), \"Tiradentes' Day\"), (datetime.date(2014, 5, 1), 'Labour Day'), (datetime.date(2014, 9, 7), 'Independence Day'), (datetime.date(2014, 10, 12), 'Our Lady of Aparecida'), (datetime.date(2014, 11, 2), \"All Souls' Day\"), (datetime.date(2014, 11, 15), 'Republic Day'), (datetime.date(2014, 12, 25), 'Christmas Day')]\n",
      "2015 [(datetime.date(2015, 1, 1), 'New year'), (datetime.date(2015, 4, 21), \"Tiradentes' Day\"), (datetime.date(2015, 5, 1), 'Labour Day'), (datetime.date(2015, 9, 7), 'Independence Day'), (datetime.date(2015, 10, 12), 'Our Lady of Aparecida'), (datetime.date(2015, 11, 2), \"All Souls' Day\"), (datetime.date(2015, 11, 15), 'Republic Day'), (datetime.date(2015, 12, 25), 'Christmas Day')]\n",
      "2016 [(datetime.date(2016, 1, 1), 'New year'), (datetime.date(2016, 4, 21), \"Tiradentes' Day\"), (datetime.date(2016, 5, 1), 'Labour Day'), (datetime.date(2016, 9, 7), 'Independence Day'), (datetime.date(2016, 10, 12), 'Our Lady of Aparecida'), (datetime.date(2016, 11, 2), \"All Souls' Day\"), (datetime.date(2016, 11, 15), 'Republic Day'), (datetime.date(2016, 12, 25), 'Christmas Day')]\n",
      "2017 [(datetime.date(2017, 1, 1), 'New year'), (datetime.date(2017, 4, 21), \"Tiradentes' Day\"), (datetime.date(2017, 5, 1), 'Labour Day'), (datetime.date(2017, 9, 7), 'Independence Day'), (datetime.date(2017, 10, 12), 'Our Lady of Aparecida'), (datetime.date(2017, 11, 2), \"All Souls' Day\"), (datetime.date(2017, 11, 15), 'Republic Day'), (datetime.date(2017, 12, 25), 'Christmas Day')]\n"
     ]
    }
   ],
   "source": [
    "cal = Brazil()\n",
    "for yr in [2011,2012,2013,2014,2015,2016,2017]:\n",
    "    print(yr,cal.holidays(yr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(datetime.date(2013, 4, 21), \"Tiradentes' Day\")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal.holidays(2013)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_hist_trans,df_new_merchant_trans]:\n",
    "    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n",
    "#     df['date'] = df['purchase_date'].dt.date\n",
    "    df['year'] = df['purchase_date'].dt.year\n",
    "    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n",
    "    df['month'] = df['purchase_date'].dt.month\n",
    "    df['dayofweek'] = df['purchase_date'].dt.dayofweek\n",
    "    df['weekend'] = (df.purchase_date.dt.weekday >=5).astype(int)\n",
    "    df['hour'] = df['purchase_date'].dt.hour\n",
    "    df['authorized_flag'] = df['authorized_flag'].map({'Y':1, 'N':0})\n",
    "    df['category_1'] = df['category_1'].map({'Y':1, 'N':0}) \n",
    "    df['month_diff'] = ((datetime.today() - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] += df['month_lag']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggs = {}\n",
    "for col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id']:\n",
    "    aggs[col] = ['nunique']\n",
    "\n",
    "aggs['purchase_amount'] = ['sum','max','min','mean','var']\n",
    "aggs['installments'] = ['sum','max','min','mean','var']\n",
    "aggs['purchase_date'] = ['max','min']\n",
    "aggs['month_lag'] = ['max','min','mean','var']\n",
    "aggs['month_diff'] = ['mean']\n",
    "aggs['authorized_flag'] = ['sum', 'mean']\n",
    "aggs['weekend'] = ['sum', 'mean']\n",
    "aggs['category_1'] = ['sum', 'mean']\n",
    "aggs['card_id'] = ['size']\n",
    "\n",
    "for col in ['category_2','category_3']:\n",
    "    df_hist_trans[col+'_mean'] = df_hist_trans.groupby([col])['purchase_amount'].transform('mean')\n",
    "    aggs[col+'_mean'] = ['mean']    \n",
    "\n",
    "new_columns = get_new_columns('hist',aggs)\n",
    "df_hist_trans_group = df_hist_trans.groupby('card_id').agg(aggs)\n",
    "df_hist_trans_group.columns = new_columns\n",
    "df_hist_trans_group.reset_index(drop=False,inplace=True)\n",
    "df_hist_trans_group['hist_purchase_date_diff'] = (df_hist_trans_group['hist_purchase_date_max'] - df_hist_trans_group['hist_purchase_date_min']).dt.days\n",
    "df_hist_trans_group['hist_purchase_date_average'] = df_hist_trans_group['hist_purchase_date_diff']/df_hist_trans_group['hist_card_id_size']\n",
    "df_hist_trans_group['hist_purchase_date_uptonow'] = (datetime.today() - df_hist_trans_group['hist_purchase_date_max']).dt.days\n",
    "df_train = df_train.merge(df_hist_trans_group,on='card_id',how='left')\n",
    "df_test = df_test.merge(df_hist_trans_group,on='card_id',how='left')\n",
    "\n",
    "del df_hist_trans_group;\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggs = {}\n",
    "for col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id']:\n",
    "    aggs[col] = ['nunique']\n",
    "aggs['purchase_amount'] = ['sum','max','min','mean','var']\n",
    "aggs['installments'] = ['sum','max','min','mean','var']\n",
    "aggs['purchase_date'] = ['max','min']\n",
    "aggs['month_lag'] = ['max','min','mean','var']\n",
    "aggs['month_diff'] = ['mean']\n",
    "aggs['weekend'] = ['sum', 'mean']\n",
    "aggs['category_1'] = ['sum', 'mean']\n",
    "aggs['card_id'] = ['size']\n",
    "\n",
    "for col in ['category_2','category_3']:\n",
    "    df_new_merchant_trans[col+'_mean'] = df_new_merchant_trans.groupby([col])['purchase_amount'].transform('mean')\n",
    "    aggs[col+'_mean'] = ['mean']\n",
    "    \n",
    "new_columns = get_new_columns('new_hist',aggs)\n",
    "df_hist_trans_group = df_new_merchant_trans.groupby('card_id').agg(aggs)\n",
    "df_hist_trans_group.columns = new_columns\n",
    "df_hist_trans_group.reset_index(drop=False,inplace=True)\n",
    "df_hist_trans_group['new_hist_purchase_date_diff'] = (df_hist_trans_group['new_hist_purchase_date_max'] - df_hist_trans_group['new_hist_purchase_date_min']).dt.days\n",
    "df_hist_trans_group['new_hist_purchase_date_average'] = df_hist_trans_group['new_hist_purchase_date_diff']/df_hist_trans_group['new_hist_card_id_size']\n",
    "df_hist_trans_group['new_hist_purchase_date_uptonow'] = (datetime.today() - df_hist_trans_group['new_hist_purchase_date_max']).dt.days\n",
    "df_train = df_train.merge(df_hist_trans_group,on='card_id',how='left')\n",
    "df_test = df_test.merge(df_hist_trans_group,on='card_id',how='left')\n",
    "\n",
    "del df_hist_trans_group;\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_active_month</th>\n",
       "      <th>card_id</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>target</th>\n",
       "      <th>hist_month_nunique</th>\n",
       "      <th>hist_hour_nunique</th>\n",
       "      <th>hist_weekofyear_nunique</th>\n",
       "      <th>hist_dayofweek_nunique</th>\n",
       "      <th>...</th>\n",
       "      <th>new_hist_weekend_sum</th>\n",
       "      <th>new_hist_weekend_mean</th>\n",
       "      <th>new_hist_category_1_sum</th>\n",
       "      <th>new_hist_category_1_mean</th>\n",
       "      <th>new_hist_card_id_size</th>\n",
       "      <th>new_hist_category_2_mean_mean</th>\n",
       "      <th>new_hist_category_3_mean_mean</th>\n",
       "      <th>new_hist_purchase_date_diff</th>\n",
       "      <th>new_hist_purchase_date_average</th>\n",
       "      <th>new_hist_purchase_date_uptonow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-06</td>\n",
       "      <td>C_ID_92a2005557</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.820312</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>35</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>-0.550293</td>\n",
       "      <td>-0.592773</td>\n",
       "      <td>54.0</td>\n",
       "      <td>2.347826</td>\n",
       "      <td>231.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01</td>\n",
       "      <td>C_ID_3d0044924f</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.392822</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.550293</td>\n",
       "      <td>-0.606445</td>\n",
       "      <td>56.0</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>262.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-08</td>\n",
       "      <td>C_ID_d639edf6cd</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.687988</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.548828</td>\n",
       "      <td>-0.592773</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>232.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-09</td>\n",
       "      <td>C_ID_186d6a6901</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142456</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.556641</td>\n",
       "      <td>-0.604492</td>\n",
       "      <td>41.0</td>\n",
       "      <td>5.857143</td>\n",
       "      <td>243.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11</td>\n",
       "      <td>C_ID_cdbd2c0db2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.159790</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>36.0</td>\n",
       "      <td>-0.555664</td>\n",
       "      <td>-0.588379</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1.583333</td>\n",
       "      <td>232.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_active_month          card_id  feature_1  feature_2  feature_3  \\\n",
       "0            2017-06  C_ID_92a2005557          5          2          1   \n",
       "1            2017-01  C_ID_3d0044924f          4          1          0   \n",
       "2            2016-08  C_ID_d639edf6cd          2          2          0   \n",
       "3            2017-09  C_ID_186d6a6901          4          3          0   \n",
       "4            2017-11  C_ID_cdbd2c0db2          1          3          0   \n",
       "\n",
       "     target  hist_month_nunique  hist_hour_nunique  hist_weekofyear_nunique  \\\n",
       "0 -0.820312                   9                 23                       35   \n",
       "1  0.392822                  12                 24                       50   \n",
       "2  0.687988                  10                 14                       22   \n",
       "3  0.142456                   6                 16                       20   \n",
       "4 -0.159790                   4                 22                       17   \n",
       "\n",
       "   hist_dayofweek_nunique               ...                \\\n",
       "0                       7               ...                 \n",
       "1                       7               ...                 \n",
       "2                       7               ...                 \n",
       "3                       7               ...                 \n",
       "4                       7               ...                 \n",
       "\n",
       "   new_hist_weekend_sum  new_hist_weekend_mean  new_hist_category_1_sum  \\\n",
       "0                   6.0               0.260870                      0.0   \n",
       "1                   0.0               0.000000                      0.0   \n",
       "2                   1.0               1.000000                      0.0   \n",
       "3                   3.0               0.428571                      1.0   \n",
       "4                  12.0               0.333333                      2.0   \n",
       "\n",
       "   new_hist_category_1_mean  new_hist_card_id_size  \\\n",
       "0                  0.000000                   23.0   \n",
       "1                  0.000000                    6.0   \n",
       "2                  0.000000                    1.0   \n",
       "3                  0.142857                    7.0   \n",
       "4                  0.055556                   36.0   \n",
       "\n",
       "   new_hist_category_2_mean_mean  new_hist_category_3_mean_mean  \\\n",
       "0                      -0.550293                      -0.592773   \n",
       "1                      -0.550293                      -0.606445   \n",
       "2                      -0.548828                      -0.592773   \n",
       "3                      -0.556641                      -0.604492   \n",
       "4                      -0.555664                      -0.588379   \n",
       "\n",
       "   new_hist_purchase_date_diff  new_hist_purchase_date_average  \\\n",
       "0                         54.0                        2.347826   \n",
       "1                         56.0                        9.333333   \n",
       "2                          0.0                        0.000000   \n",
       "3                         41.0                        5.857143   \n",
       "4                         57.0                        1.583333   \n",
       "\n",
       "   new_hist_purchase_date_uptonow  \n",
       "0                           231.0  \n",
       "1                           262.0  \n",
       "2                           232.0  \n",
       "3                           243.0  \n",
       "4                           232.0  \n",
       "\n",
       "[5 rows x 78 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df_hist_trans;\n",
    "gc.collect()\n",
    "\n",
    "del df_new_merchant_trans;\n",
    "gc.collect()\n",
    "\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    199710\n",
       "1      2207\n",
       "Name: outliers, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['outliers'] = 0\n",
    "df_train.loc[df_train['target'] < -30, 'outliers'] = 1\n",
    "df_train['outliers'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with the one nan in df_test.first_active_month a bit arbitrarily for now\n",
    "df_test.loc[df_test['first_active_month'].isna(),'first_active_month'] = df_test.iloc[11577]['first_active_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_train,df_test]:\n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "    df['dayofweek'] = df['first_active_month'].dt.dayofweek\n",
    "    df['weekofyear'] = df['first_active_month'].dt.weekofyear\n",
    "    df['month'] = df['first_active_month'].dt.month\n",
    "    df['elapsed_time'] = (datetime.today() - df['first_active_month']).dt.days\n",
    "    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['new_hist_first_buy'] = (df['new_hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    for f in ['hist_purchase_date_max','hist_purchase_date_min','new_hist_purchase_date_max',\\\n",
    "                     'new_hist_purchase_date_min']:\n",
    "        df[f] = df[f].astype(np.int64) * 1e-9\n",
    "    df['card_id_total'] = df['new_hist_card_id_size']+df['hist_card_id_size']\n",
    "    df['purchase_amount_total'] = df['new_hist_purchase_amount_sum']+df['hist_purchase_amount_sum']\n",
    "    \n",
    "    df['date'] = df['first_active_month'].dt.date\n",
    "    \n",
    "     # These are the 8 added features, calculating the no of working days between the first active month and each of the 8 standard Brailian holidays\n",
    "        \n",
    "    df['day_diff1'] = df['date'].apply(lambda x: cal.get_working_days_delta(x,cal.holidays(int(x.year))[0][0])) # have to make this less clunky, write a function\n",
    "    df['day_diff2'] = df['date'].apply(lambda x: cal.get_working_days_delta(x,cal.holidays(int(x.year))[1][0]))\n",
    "    df['day_diff3'] = df['date'].apply(lambda x: cal.get_working_days_delta(x,cal.holidays(int(x.year))[2][0]))\n",
    "    df['day_diff4'] = df['date'].apply(lambda x: cal.get_working_days_delta(x,cal.holidays(int(x.year))[3][0]))\n",
    "    df['day_diff5'] = df['date'].apply(lambda x: cal.get_working_days_delta(x,cal.holidays(int(x.year))[4][0]))\n",
    "    df['day_diff6'] = df['date'].apply(lambda x: cal.get_working_days_delta(x,cal.holidays(int(x.year))[5][0]))\n",
    "    df['day_diff7'] = df['date'].apply(lambda x: cal.get_working_days_delta(x,cal.holidays(int(x.year))[6][0]))\n",
    "    df['day_diff8'] = df['date'].apply(lambda x: cal.get_working_days_delta(x,cal.holidays(int(x.year))[7][0]))\n",
    "    \n",
    "    df.drop(['date'],axis=1,inplace=True)\n",
    "    \n",
    "for f in ['feature_1','feature_2','feature_3']:\n",
    "    order_label = df_train.groupby([f])['outliers'].mean()\n",
    "    df_train[f] = df_train[f].map(order_label)\n",
    "    df_test[f] = df_test[f].map(order_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_columns = [c for c in df_train.columns if c not in ['card_id', 'first_active_month','target','outliers']]\n",
    "target = df_train['target']\n",
    "del df_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 3.66329\tvalid_1's rmse: 3.73431\n",
      "[200]\ttraining's rmse: 3.58581\tvalid_1's rmse: 3.70425\n",
      "[300]\ttraining's rmse: 3.53778\tvalid_1's rmse: 3.69175\n",
      "[400]\ttraining's rmse: 3.50239\tvalid_1's rmse: 3.68542\n",
      "[500]\ttraining's rmse: 3.47176\tvalid_1's rmse: 3.68051\n",
      "[600]\ttraining's rmse: 3.44677\tvalid_1's rmse: 3.67742\n",
      "[700]\ttraining's rmse: 3.42468\tvalid_1's rmse: 3.67555\n",
      "[800]\ttraining's rmse: 3.40456\tvalid_1's rmse: 3.674\n",
      "[900]\ttraining's rmse: 3.38505\tvalid_1's rmse: 3.67345\n",
      "[1000]\ttraining's rmse: 3.3691\tvalid_1's rmse: 3.67272\n",
      "[1100]\ttraining's rmse: 3.3516\tvalid_1's rmse: 3.67229\n",
      "Early stopping, best iteration is:\n",
      "[1082]\ttraining's rmse: 3.35463\tvalid_1's rmse: 3.67222\n",
      "fold 1\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 3.67179\tvalid_1's rmse: 3.70106\n",
      "[200]\ttraining's rmse: 3.59577\tvalid_1's rmse: 3.66598\n",
      "[300]\ttraining's rmse: 3.54964\tvalid_1's rmse: 3.65203\n",
      "[400]\ttraining's rmse: 3.51332\tvalid_1's rmse: 3.64309\n",
      "[500]\ttraining's rmse: 3.4847\tvalid_1's rmse: 3.63866\n",
      "[600]\ttraining's rmse: 3.45993\tvalid_1's rmse: 3.63584\n",
      "[700]\ttraining's rmse: 3.43849\tvalid_1's rmse: 3.6335\n",
      "[800]\ttraining's rmse: 3.41755\tvalid_1's rmse: 3.63186\n",
      "[900]\ttraining's rmse: 3.39848\tvalid_1's rmse: 3.63091\n",
      "[1000]\ttraining's rmse: 3.38145\tvalid_1's rmse: 3.63014\n",
      "[1100]\ttraining's rmse: 3.36532\tvalid_1's rmse: 3.62996\n",
      "[1200]\ttraining's rmse: 3.34969\tvalid_1's rmse: 3.6297\n",
      "Early stopping, best iteration is:\n",
      "[1193]\ttraining's rmse: 3.35086\tvalid_1's rmse: 3.6296\n",
      "fold 2\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 3.66402\tvalid_1's rmse: 3.72295\n",
      "[200]\ttraining's rmse: 3.58603\tvalid_1's rmse: 3.69242\n",
      "[300]\ttraining's rmse: 3.53763\tvalid_1's rmse: 3.6823\n",
      "[400]\ttraining's rmse: 3.5023\tvalid_1's rmse: 3.67657\n",
      "[500]\ttraining's rmse: 3.47207\tvalid_1's rmse: 3.67199\n",
      "[600]\ttraining's rmse: 3.44712\tvalid_1's rmse: 3.66927\n",
      "[700]\ttraining's rmse: 3.42427\tvalid_1's rmse: 3.66865\n",
      "[800]\ttraining's rmse: 3.40289\tvalid_1's rmse: 3.66848\n",
      "[900]\ttraining's rmse: 3.38372\tvalid_1's rmse: 3.66774\n",
      "[1000]\ttraining's rmse: 3.36545\tvalid_1's rmse: 3.66786\n",
      "Early stopping, best iteration is:\n",
      "[922]\ttraining's rmse: 3.3798\tvalid_1's rmse: 3.66758\n",
      "fold 3\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 3.66741\tvalid_1's rmse: 3.71518\n",
      "[200]\ttraining's rmse: 3.58924\tvalid_1's rmse: 3.68638\n",
      "[300]\ttraining's rmse: 3.54098\tvalid_1's rmse: 3.67384\n",
      "[400]\ttraining's rmse: 3.50526\tvalid_1's rmse: 3.66684\n",
      "[500]\ttraining's rmse: 3.47524\tvalid_1's rmse: 3.66231\n",
      "[600]\ttraining's rmse: 3.44947\tvalid_1's rmse: 3.65942\n",
      "[700]\ttraining's rmse: 3.42579\tvalid_1's rmse: 3.6583\n",
      "[800]\ttraining's rmse: 3.40502\tvalid_1's rmse: 3.65725\n",
      "[900]\ttraining's rmse: 3.38637\tvalid_1's rmse: 3.65678\n",
      "[1000]\ttraining's rmse: 3.36876\tvalid_1's rmse: 3.65626\n",
      "[1100]\ttraining's rmse: 3.35221\tvalid_1's rmse: 3.65618\n",
      "[1200]\ttraining's rmse: 3.33633\tvalid_1's rmse: 3.65599\n",
      "Early stopping, best iteration is:\n",
      "[1188]\ttraining's rmse: 3.33805\tvalid_1's rmse: 3.65585\n",
      "fold 4\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining's rmse: 3.66654\tvalid_1's rmse: 3.70985\n",
      "[200]\ttraining's rmse: 3.58756\tvalid_1's rmse: 3.68336\n",
      "[300]\ttraining's rmse: 3.54004\tvalid_1's rmse: 3.67208\n",
      "[400]\ttraining's rmse: 3.50322\tvalid_1's rmse: 3.66475\n",
      "[500]\ttraining's rmse: 3.47315\tvalid_1's rmse: 3.66073\n",
      "[600]\ttraining's rmse: 3.44697\tvalid_1's rmse: 3.65922\n",
      "[700]\ttraining's rmse: 3.42403\tvalid_1's rmse: 3.658\n",
      "[800]\ttraining's rmse: 3.40323\tvalid_1's rmse: 3.65753\n",
      "[900]\ttraining's rmse: 3.38457\tvalid_1's rmse: 3.65733\n",
      "[1000]\ttraining's rmse: 3.36658\tvalid_1's rmse: 3.65701\n",
      "[1100]\ttraining's rmse: 3.34962\tvalid_1's rmse: 3.65663\n",
      "[1200]\ttraining's rmse: 3.334\tvalid_1's rmse: 3.65625\n",
      "[1300]\ttraining's rmse: 3.31864\tvalid_1's rmse: 3.65608\n",
      "[1400]\ttraining's rmse: 3.30374\tvalid_1's rmse: 3.65612\n",
      "[1500]\ttraining's rmse: 3.28942\tvalid_1's rmse: 3.65596\n",
      "Early stopping, best iteration is:\n",
      "[1470]\ttraining's rmse: 3.2935\tvalid_1's rmse: 3.6557\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.6562194146442479"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param = {'num_leaves': 31,\n",
    "         'min_data_in_leaf': 30, \n",
    "         'objective':'regression',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.01,\n",
    "         \"min_child_samples\": 20,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 11,\n",
    "         \"metric\": 'rmse',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1,\n",
    "         \"nthread\": 4,\n",
    "         \"random_state\": 4590}\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=4590)\n",
    "oof = np.zeros(len(df_train))\n",
    "predictions = np.zeros(len(df_test))\n",
    "feature_importance_df = pd.DataFrame()\n",
    "\n",
    "for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train,df_train['outliers'].values)):\n",
    "    print(\"fold {}\".format(fold_))\n",
    "    trn_data = lgb.Dataset(df_train.iloc[trn_idx][df_train_columns], label=target.iloc[trn_idx])#, categorical_feature=categorical_feats)\n",
    "    val_data = lgb.Dataset(df_train.iloc[val_idx][df_train_columns], label=target.iloc[val_idx])#, categorical_feature=categorical_feats)\n",
    "\n",
    "    num_round = 10000\n",
    "    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 100)\n",
    "    oof[val_idx] = clf.predict(df_train.iloc[val_idx][df_train_columns], num_iteration=clf.best_iteration)\n",
    "    \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"Feature\"] = df_train_columns\n",
    "    fold_importance_df[\"importance\"] = clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold_ + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    predictions += clf.predict(df_test[df_train_columns], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "np.sqrt(mean_squared_error(oof, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
