{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6df55b542f152882e00385a0f73198f4e3bc4316"
   },
   "source": [
    "**FEEL FREE TO UPVOTE**  （＾ｖ＾）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b00db406434116eaef1206208331f97fe0542872"
   },
   "outputs": [],
   "source": [
    "#使用我的特征，队友的思路识别异常点，然后进行手动修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(4950)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "337eacd2e6e13207d06cb90f9f8cd352d300b323"
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../input/train.csv')\n",
    "df_test = pd.read_csv('../input/test.csv')\n",
    "df_hist_trans = pd.read_csv('../input/historical_transactions.csv')\n",
    "df_new_merchant_trans = pd.read_csv('../input/new_merchant_transactions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "252db1c4842288dc2c4c5a116daa5674073c6daf"
   },
   "outputs": [],
   "source": [
    "df_hist_trans = reduce_mem_usage(df_hist_trans)\n",
    "df_new_merchant_trans = reduce_mem_usage(df_new_merchant_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "71f89a3b8a93b2f2feb2cd0a45f860cde33687be"
   },
   "outputs": [],
   "source": [
    "for df in [df_hist_trans,df_new_merchant_trans]:\n",
    "#     df['category_2'].fillna(1.0,inplace=True)\n",
    "#     df['category_3'].fillna('A',inplace=True)\n",
    "#     df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n",
    "    #修改特征\n",
    "    df['category_2'].fillna(-1,inplace=True)\n",
    "    df['category_3'].fillna('other',inplace=True)\n",
    "    df['merchant_id'].fillna('other',inplace=True)\n",
    "#     df.loc[df['installments'].isin([999,-1]),'installments'] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dda90662d05e22310dd713df106ea07f4b8bccfc"
   },
   "outputs": [],
   "source": [
    "def get_new_columns(name,aggs):\n",
    "    #for for 写法 nice\n",
    "    return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "690ba01a38f524e9345b419200f588f937bc067a"
   },
   "outputs": [],
   "source": [
    "for df in [df_hist_trans,df_new_merchant_trans]:\n",
    "    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n",
    "    df['year'] = df['purchase_date'].dt.year\n",
    "    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n",
    "    df['month'] = df['purchase_date'].dt.month\n",
    "    df['dayofweek'] = df['purchase_date'].dt.dayofweek\n",
    "    df['weekend'] = (df.purchase_date.dt.weekday >=5).astype(int)\n",
    "    df['hour'] = df['purchase_date'].dt.hour\n",
    "    df['authorized_flag'] = df['authorized_flag'].map({'Y':1, 'N':0})\n",
    "    df['category_1'] = df['category_1'].map({'Y':1, 'N':0}) \n",
    "    #https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/73244\n",
    "    df['month_diff'] = ((datetime.datetime(2018,6,1) - df['purchase_date']).dt.days)//30\n",
    "    df['month_diff'] += df['month_lag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "80aadb70cf47be8cf45b681d73214ace43fe8d1d"
   },
   "outputs": [],
   "source": [
    "#因为根据auth_flag 将特征分成了两部分，这里聚合一个全局的auth_flag 特征\n",
    "aggs = {}\n",
    "aggs['purchase_amount'] = ['sum']\n",
    "# aggs['installments'] = ['sum','max','min','mean','var','median']\n",
    "aggs['authorized_flag'] = ['sum', 'mean','std']\n",
    "aggs['card_id'] = ['size']\n",
    "auth_flag = df_hist_trans.groupby(['card_id']).agg(aggs)\n",
    "auth_flag.columns = get_new_columns('auth_flag',aggs)\n",
    "auth_flag.reset_index(inplace=True)\n",
    "df_train = df_train.merge(auth_flag,on='card_id',how='left')\n",
    "df_test = df_test.merge(auth_flag,on='card_id',how='left')\n",
    "del auth_flag\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "49db45166453de5e0509d5e43d52604344954744"
   },
   "outputs": [],
   "source": [
    "#对 authorized_flag进行结果编码\n",
    "aggs = {}\n",
    "for col in ['category_2','category_3','state_id','subsector_id','merchant_category_id','city_id']:\n",
    "    df_hist_trans[col+'_auth_mean'] = df_hist_trans.groupby([col])['authorized_flag'].transform('mean')\n",
    "    df_hist_trans[col+'_auth_sum'] = df_hist_trans.groupby([col])['authorized_flag'].transform('sum') \n",
    "    aggs[col+'_auth_mean'] = ['mean']\n",
    "    aggs[col+'_auth_sum'] = ['sum'] \n",
    "auth_encoder = df_hist_trans.groupby(['card_id']).agg(aggs)\n",
    "auth_encoder.columns = get_new_columns('auth_encoder',aggs)\n",
    "auth_encoder.reset_index(inplace=True)\n",
    "df_train = df_train.merge(auth_encoder,on='card_id',how='left')\n",
    "df_test = df_test.merge(auth_encoder,on='card_id',how='left')\n",
    "del auth_encoder\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5b08ce613977fc59acfdac57c208ba71db6a5ea3"
   },
   "outputs": [],
   "source": [
    "def aggregate_per_month(prefix,history,agg_func):\n",
    "    grouped = history.groupby(['card_id', 'month_lag'])\n",
    "    intermediate_group = grouped.agg(agg_func)\n",
    "    intermediate_group.columns = [prefix + '_'.join(col).strip() for col in intermediate_group.columns.values]\n",
    "    intermediate_group.reset_index(inplace=True)\n",
    "\n",
    "    final_group = intermediate_group.groupby('card_id').agg(['mean', 'std'])\n",
    "    final_group.columns = [prefix + '_'.join(col).strip() for col in final_group.columns.values]\n",
    "    final_group.reset_index(inplace=True) \n",
    "    return final_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "508c91c2f30371a3bb5a8e5daac36f43749c55c7"
   },
   "outputs": [],
   "source": [
    "#对授权码 进行按月聚合\n",
    "agg_func = {'authorized_flag': [ 'sum', 'mean','median']}\n",
    "final_group =  aggregate_per_month('agg_per_month_total',df_hist_trans,agg_func) \n",
    "df_train = df_train.merge(final_group,on='card_id',how='left')\n",
    "df_test = df_test.merge(final_group,on='card_id',how='left')\n",
    "del final_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6dfd525f8227507cad2d8d3a6144bb1708f7e462"
   },
   "outputs": [],
   "source": [
    "authorized_transactions = df_hist_trans[df_hist_trans['authorized_flag'] == 1]\n",
    "df_hist_trans = df_hist_trans[df_hist_trans['authorized_flag'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3102d6a23f76d423d622e2177aea1dcc19abdbb4"
   },
   "outputs": [],
   "source": [
    "#使用countVectorizer对category特征进行处理，别人号称可以提升 千分之3\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# cv = CountVectorizer(token_pattern='\\w{1,}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "22f8b5af830b7591197956f40b8b575d1e75c713"
   },
   "outputs": [],
   "source": [
    "df_hist_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "53204f1e1752f4e51d96e7c05360898ababc655d"
   },
   "outputs": [],
   "source": [
    "agg_func = {'purchase_amount': [ 'sum', 'mean', 'min', 'max', 'std']}\n",
    "final_group =  aggregate_per_month('agg_per_month_auth',authorized_transactions,agg_func) \n",
    "df_train = df_train.merge(final_group,on='card_id',how='left')\n",
    "df_test = df_test.merge(final_group,on='card_id',how='left')\n",
    "del final_group\n",
    "final_group =  aggregate_per_month('agg_per_month_hist',df_hist_trans,agg_func) \n",
    "df_train = df_train.merge(final_group,on='card_id',how='left')\n",
    "df_test = df_test.merge(final_group,on='card_id',how='left')\n",
    "del final_group\n",
    "final_group =  aggregate_per_month('agg_per_month_hist_new',df_new_merchant_trans,agg_func) \n",
    "df_train = df_train.merge(final_group,on='card_id',how='left')\n",
    "df_test = df_test.merge(final_group,on='card_id',how='left')\n",
    "del final_group\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6bc6f16f1dbf38bcb45a9345fd1e789b23f0efb2"
   },
   "outputs": [],
   "source": [
    "df_train['outliers'] = 0\n",
    "df_train.loc[df_train['target'] < -30, 'outliers'] = 1\n",
    "df_train['outliers'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ddf1d5bb0ade2b22b0f072c208c1506ea64503ea"
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for df in [authorized_transactions,df_hist_trans]:\n",
    "    aggs = {}\n",
    "    for col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id',\n",
    "                'state_id','city_id']:\n",
    "        aggs[col] = ['nunique']\n",
    "\n",
    "    aggs['purchase_amount'] = ['sum','max','min','mean','var','median']\n",
    "    aggs['installments'] = ['sum','max','min','mean','var','median']\n",
    "    aggs['purchase_date'] = ['max','min']\n",
    "    aggs['month_lag'] = ['max','min','mean','var','median']\n",
    "    aggs['month_diff'] = ['mean','median']\n",
    "#     aggs['authorized_flag'] = ['sum', 'mean','median']\n",
    "    aggs['weekend'] = ['sum', 'mean']\n",
    "    aggs['category_1'] = ['sum', 'mean']\n",
    "    aggs['card_id'] = ['size']\n",
    "    #产生交叉特征，内存有问题\n",
    "    features = ['category_2','category_3','state_id','subsector_id','merchant_category_id','city_id'\n",
    "               ,'merchant_id']\n",
    "#     for coli in ['category_2','category_3','state_id','subsector_id','merchant_category_id','city_id']:\n",
    "#         for colj in ['category_2','category_3','state_id','subsector_id','merchant_category_id','city_id']:\n",
    "#             df[coli + colj] = df[coli].astype(str) + df[colj].astype(str)\n",
    "#             features.append(coli + colj)\n",
    "    for col in features:\n",
    "        df[col+'_mean'] = df.groupby([col])['purchase_amount'].transform('mean')\n",
    "        df[col+'_sum'] = df.groupby([col])['purchase_amount'].transform('sum') \n",
    "        aggs[col+'_mean'] = ['mean']\n",
    "        aggs[col+'_sum'] = ['sum'] \n",
    "#         添加特征，使用outlier进行编码\n",
    "#         outliers_mean = df.groupby([col])['outliers'].mean()\n",
    "#         outliers_sum = df.groupby([col])['outliers'].sum()\n",
    "#         df[col+'_outliers_mean'] = df[col].map(outliers_mean)\n",
    "#         df[col+'_outliers_sum'] = df[col].map(outliers_sum)\n",
    "#         aggs[col+'_outliers_mean'] = ['mean']\n",
    "#         aggs[col+'_outliers_sum'] =['sum']   \n",
    "        \n",
    "    if i == 0:\n",
    "        prefix = 'auth_hist'\n",
    "    else:\n",
    "        prefix = 'hist'\n",
    "    new_columns = get_new_columns(prefix,aggs)\n",
    "    i += 1\n",
    "    # df_hist_trans.sort_values(['card_id','purchase_date'],inplace = True)\n",
    "\n",
    "    df_hist_trans_group = df.groupby('card_id').agg(aggs)\n",
    "    df_hist_trans_group.columns = new_columns\n",
    "    df_hist_trans_group.reset_index(drop=False,inplace=True)\n",
    "    df_hist_trans_group[prefix + '_purchase_date_diff'] = (df_hist_trans_group[prefix + '_purchase_date_max'] - df_hist_trans_group[prefix + '_purchase_date_min']).dt.days\n",
    "    df_hist_trans_group[prefix + '_purchase_date_average'] = df_hist_trans_group[prefix + '_purchase_date_diff']/df_hist_trans_group[prefix + '_card_id_size']\n",
    "    df_hist_trans_group[prefix + '_purchase_date_uptonow'] = (datetime.datetime(2018,6,1) - df_hist_trans_group[prefix + '_purchase_date_max']).dt.days\n",
    "    #下面这个特征考虑了：有的人可能购买频率较低，但是还是忠实粉丝的情况\n",
    "#     df_hist_trans_group[prefix + '_purchase_date_uptonow_ave'] =  df_hist_trans_group[prefix + '_purchase_date_uptonow']/df_hist_trans_group[prefix + '_purchase_date_average']\n",
    "\n",
    "    #每一个card中未授权消费次数\n",
    "#     df_hist_trans_group[prefix + '_unauthorized_number'] = df_hist_trans_group[prefix + '_card_id_size'] - df_hist_trans_group[prefix + '_authorized_flag_sum']\n",
    "    #最近活跃时间，确实是一个强特征，感觉可以再挖出来几个特征，比如最近5次消费时间，最近10次消费时间，如果值比较小，说明最近很活跃\n",
    "    #没有效果\n",
    "    # grouped =  df_hist_trans.groupby('card_id')['purchase_date']\n",
    "    # df_hist_trans_group['hist_purchase_5thdate_uptonow'] =  (datetime.datetime.today() -grouped.shift(5)).dt.days\n",
    "    # df_hist_trans_group['hist_purchase_3thdate_uptonow'] =  (datetime.datetime.today() -grouped.shift(3)).dt.days\n",
    "    # df_hist_trans_group['hist_purchase_10thdate_uptonow'] =  (datetime.datetime.today() -grouped.shift(10)).dt.days\n",
    "    df_train = df_train.merge(df_hist_trans_group,on='card_id',how='left')\n",
    "    df_test = df_test.merge(df_hist_trans_group,on='card_id',how='left')\n",
    "    del df_hist_trans_group\n",
    "    time.sleep(5)\n",
    "    gc.collect()\n",
    "    del df;gc.collect()\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f7f5625db40db4395374991124fb796c9decd60b"
   },
   "outputs": [],
   "source": [
    "aggs = {}\n",
    "#添加特征\n",
    "for col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id',\n",
    "            'state_id','city_id']:\n",
    "    aggs[col] = ['nunique']\n",
    "    \n",
    "aggs['purchase_amount'] = ['sum','max','min','mean','var','median']\n",
    "aggs['installments'] = ['sum','max','min','mean','var','median']\n",
    "aggs['purchase_date'] = ['max','min']\n",
    "aggs['month_lag'] = ['max','min','mean','var','median']\n",
    "aggs['month_diff'] = ['mean','median']\n",
    "aggs['weekend'] = ['sum', 'mean']\n",
    "aggs['authorized_flag'] = ['sum', 'mean','median']\n",
    "aggs['category_1'] = ['sum', 'mean']\n",
    "aggs['card_id'] = ['size']\n",
    "\n",
    "#添加特征   \n",
    "features = ['category_2','category_3','state_id','subsector_id','merchant_category_id','city_id'\n",
    "               ,'merchant_id']\n",
    "#产生交叉特征，内存有问题\n",
    "# for coli in ['category_2','category_3','state_id','subsector_id','merchant_category_id','city_id']:\n",
    "#     for colj in ['category_2','category_3','state_id','subsector_id','merchant_category_id','city_id']:\n",
    "#         df_new_merchant_trans[coli + colj] = df_new_merchant_trans[coli].astype(str) + df_new_merchant_trans[colj].astype(str)\n",
    "#         features.append(coli + colj)\n",
    "for col in features:\n",
    "    df_new_merchant_trans[col+'_mean'] = df_new_merchant_trans.groupby([col])['purchase_amount'].transform('mean')\n",
    "    df_new_merchant_trans[col+'_sum'] = df_new_merchant_trans.groupby([col])['purchase_amount'].transform('sum')\n",
    "    aggs[col+'_mean'] = ['mean']\n",
    "    aggs[col+'_sum'] = ['sum']\n",
    "    #添加特征，使用outlier进行编码\n",
    "#     outliers_mean = df.groupby([col])['outliers'].mean()\n",
    "#     outliers_sum = df.groupby([col])['outliers'].sum()\n",
    "#     df[col+'_outliers_mean'] = df[col].map(outliers_mean)\n",
    "#     df[col+'_outliers_sum'] = df[col].map(outliers_sum)\n",
    "#     aggs[col+'_outliers_mean'] = ['mean']\n",
    "#     aggs[col+'_outliers_sum'] =['sum']   \n",
    "    \n",
    "new_columns = get_new_columns('new_hist',aggs)\n",
    "# df_new_merchant_trans.sort_values(['card_id','purchase_date'],inplace = True)\n",
    "df_hist_trans_group = df_new_merchant_trans.groupby('card_id').agg(aggs)\n",
    "df_hist_trans_group.columns = new_columns\n",
    "df_hist_trans_group.reset_index(drop=False,inplace=True)\n",
    "df_hist_trans_group['new_hist_purchase_date_diff'] = (df_hist_trans_group['new_hist_purchase_date_max'] - df_hist_trans_group['new_hist_purchase_date_min']).dt.days\n",
    "df_hist_trans_group['new_hist_purchase_date_average'] = df_hist_trans_group['new_hist_purchase_date_diff']/df_hist_trans_group['new_hist_card_id_size']\n",
    "df_hist_trans_group['new_hist_purchase_date_uptonow'] = (datetime.datetime(2018,6,1) - df_hist_trans_group['new_hist_purchase_date_max']).dt.days\n",
    "#下面这个特征考虑了：有的人可能购买频率较低，但是还是忠实粉丝的情况\n",
    "# df_hist_trans_group['new_hist_purchase_date_uptonow_ave'] =  df_hist_trans_group['new_hist_purchase_date_uptonow']/df_hist_trans_group['new_hist_purchase_date_average']\n",
    "\n",
    "#每一个card中未授权消费次数\n",
    "df_hist_trans_group['new_hist_unauthorized_number'] = df_hist_trans_group['new_hist_card_id_size'] - df_hist_trans_group['new_hist_authorized_flag_sum']\n",
    "# grouped = df_new_merchant_trans.groupby('card_id')['purchase_date']\n",
    "# df_hist_trans_group['new_hist_purchase_5thdate_uptonow'] =  (datetime.datetime.today() - grouped.shift(5)).dt.days\n",
    "# df_hist_trans_group['new_hist_purchase_3thdate_uptonow'] =  (datetime.datetime.today() - grouped.shift(3)).dt.days\n",
    "# df_hist_trans_group['new_hist_purchase_10thdate_uptonow'] =  (datetime.datetime.today() - grouped.shift(10)).dt.days\n",
    "\n",
    "df_train = df_train.merge(df_hist_trans_group,on='card_id',how='left')\n",
    "df_test = df_test.merge(df_hist_trans_group,on='card_id',how='left')\n",
    "del df_hist_trans_group\n",
    "gc.collect()\n",
    "del df_new_merchant_trans\n",
    "gc.collect()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ce2082fc1fb0e3f8f7d27fc166aa7a8351b65504"
   },
   "outputs": [],
   "source": [
    "for df in [df_train,df_test]:\n",
    "    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n",
    "    df['dayofweek'] = df['first_active_month'].dt.dayofweek\n",
    "    df['weekofyear'] = df['first_active_month'].dt.weekofyear\n",
    "    df['month'] = df['first_active_month'].dt.month\n",
    "    df['elapsed_time'] = (datetime.datetime(2018,6,1) - df['first_active_month']).dt.days\n",
    "    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    df['new_hist_first_buy'] = (df['new_hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    #添加特征\n",
    "    df['auth_hist_first_buy'] = (df['auth_hist_purchase_date_min'] - df['first_active_month']).dt.days\n",
    "    #修改特征\n",
    "    for f in ['hist_purchase_date_max','hist_purchase_date_min','new_hist_purchase_date_max',\\\n",
    "                     'new_hist_purchase_date_min','auth_hist_purchase_date_max','auth_hist_purchase_date_min']:\n",
    "#         del df[f]\n",
    "        df[f] = df[f].astype(np.int64) * 1e-9\n",
    "    for f in ['auth_encoder_category_2_auth_sum_sum','auth_encoder_category_3_auth_sum_sum',\n",
    "            'auth_encoder_state_id_auth_sum_sum','auth_encoder_subsector_id_auth_sum_sum',\n",
    "            'auth_encoder_merchant_category_id_auth_sum_sum','auth_encoder_city_id_auth_sum_sum']:\n",
    "        df[f] = df[f].astype(np.int64) * 1e-9\n",
    "#         del df[f]\n",
    "    #上面auth_flag已经聚合过了card_id_size ,purchase_amount\n",
    "#     df['card_id_total'] = df['new_hist_card_id_size']+df['hist_card_id_size']  + df['auth_hist_card_id_size']\n",
    "#     df['purchase_amount_total'] = df['new_hist_purchase_amount_sum']+df['hist_purchase_amount_sum']+df['auth_hist_purchase_amount_sum']\n",
    "#添加特征\n",
    "\n",
    "for f in ['feature_1','feature_2','feature_3','month','dayofweek']:\n",
    "    order_label1 = df_train.groupby([f])['outliers'].mean()\n",
    "    df_train[f+'_outliers_mean'] = df_train[f].map(order_label1)\n",
    "    df_test[f+'_outliers_mean'] = df_test[f].map(order_label1)\n",
    "    \n",
    "    order_label2 = df_train.groupby([f])['outliers'].sum()\n",
    "    df_train[f+'_outliers_sum'] = df_train[f].map(order_label2)\n",
    "    df_test[f+'_outliers_sum'] = df_test[f].map(order_label2)\n",
    "    \n",
    "#     order_label1 = df_train.groupby([f])['target'].mean()\n",
    "#     df_train[f+'_target_mean'] = df_train[f].map(order_label1)\n",
    "#     df_test[f+'_target_sum'] = df_test[f].map(order_label1)\n",
    "#     order_label2 = df_train.gorupby([f])['target'].sum()\n",
    "#     df_train[f+'_target_sum'] = df_train[f].map(order_label2)\n",
    "#     df_test[f+'_target_sum'] = df_test[f].map(order_label2)\n",
    " \n",
    "# get_dummies 似乎有一点点不良影响\n",
    "df_train = pd.get_dummies(df_train,columns =['feature_1','feature_2'])\n",
    "df_test = pd.get_dummies(df_test,columns =['feature_1','feature_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3877e4f1418facb4da080ba31ef8ebae1724e7b1"
   },
   "outputs": [],
   "source": [
    "#首次购买的时间居然早于首次激活的时间，进行调整\n",
    "df_train.loc[df_train['auth_hist_first_buy'] < 0, 'auth_hist_first_buy'] = -1\n",
    "df_train.loc[df_train['hist_first_buy'] < 0, 'hist_first_buy'] = -1\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c4f20f27679889542acfd60d1f1ac381b201ac43"
   },
   "outputs": [],
   "source": [
    "exclude_features = []\n",
    "exclude_features += ['card_id', 'first_active_month','target','outliers']\n",
    "df_train_columns = [c for c in df_train.columns if c not in exclude_features]\n",
    "target = df_train['target']\n",
    "del df_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b7ff07b98323cf30f11ec6130f5c64e6c4db0aac"
   },
   "outputs": [],
   "source": [
    "len(df_train_columns)\n",
    "len(exclude_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c9bbc95244978b519d94131907b547c2b6c94191"
   },
   "outputs": [],
   "source": [
    "param = {'num_leaves': 31,\n",
    "         'min_data_in_leaf': 32, \n",
    "         'objective':'binary',\n",
    "         'max_depth': -1,\n",
    "         'learning_rate': 0.01,\n",
    "         \"min_child_samples\": 20,\n",
    "         \"boosting\": \"gbdt\",\n",
    "         \"feature_fraction\": 0.9,\n",
    "         \"bagging_freq\": 1,\n",
    "         \"bagging_fraction\": 0.9 ,\n",
    "         \"bagging_seed\": 42,\n",
    "         \"metric\": 'auc',\n",
    "         \"lambda_l1\": 0.1,\n",
    "         \"verbosity\": -1,\n",
    "         \"nthread\": 48,\n",
    "         \"scale_pos_weight\": 15,\n",
    "         \"random_state\": 4950}\n",
    "# folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=4950)\n",
    "# oof = np.zeros(len(df_train))\n",
    "# predictions = np.zeros(len(df_test))\n",
    "\n",
    "# for fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train,df_train['outliers'].values)):\n",
    "#     print(\"fold {}\".format(fold_))\n",
    "#     trn_data = lgb.Dataset(df_train.iloc[trn_idx][df_train_columns], label=df_train['outliers'].iloc[trn_idx])#, categorical_feature=categorical_feats)\n",
    "#     val_data = lgb.Dataset(df_train.iloc[val_idx][df_train_columns], label=df_train['outliers'].iloc[val_idx])#, categorical_feature=categorical_feats)\n",
    "\n",
    "#     num_round = 10000\n",
    "#     clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 100)\n",
    "#     oof[val_idx] = clf.predict(df_train.iloc[val_idx][df_train_columns], num_iteration=clf.best_iteration)\n",
    "    \n",
    "#     predictions += clf.predict(df_test[df_train_columns], num_iteration=clf.best_iteration) / folds.n_splits\n",
    "\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# oof_label = [1 if c > 0.5 else 0 for c in oof]\n",
    "# roc_auc_score(oof_label, df_train['outliers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c9b82f85e5a42afcf3ab391b41278be89b7c606"
   },
   "outputs": [],
   "source": [
    "#使用下面的方法进行交叉验证确定最佳 boosting round 500\n",
    "trn_data = lgb.Dataset(df_train[df_train_columns], label=df_train['outliers'].values)#, categorical_feature=categorical_feats)\n",
    "lgb_cv = lgb.cv(param, trn_data, 10000, early_stopping_rounds=600, verbose_eval=200, stratified=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "337271cd40d891583ac62f9ff2976215613aeb46"
   },
   "outputs": [],
   "source": [
    "clf = lgb.train(param, trn_data, 500, verbose_eval=100, valid_sets=(trn_data))\n",
    "te_prob = clf.predict(df_test[df_train_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "355e9c24949b8e5d677fe5a2f117228c3310dab6"
   },
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\n",
    "sub_df[\"target\"] = te_prob\n",
    "sub_df.to_csv(\"outlier_prob.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
