{
  "cells": [
    {
      "metadata": {
        "_uuid": "6df55b542f152882e00385a0f73198f4e3bc4316"
      },
      "cell_type": "markdown",
      "source": "**FEEL FREE TO UPVOTE**  （＾ｖ＾）"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b00db406434116eaef1206208331f97fe0542872"
      },
      "cell_type": "code",
      "source": "#\n#目前最好的结果v12，使用三个模型分别预测，取得了3.684的成绩\n#分类预测\n#v20提交成绩得到了3.688， cv 成绩3.6519\n#使用authorized_flag 进行特征分别处理,收到了奇效\n#使用countVectorizer 对类别变量进行编码，内存存在压力",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport datetime\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold,KFold\nfrom sklearn.metrics import mean_squared_error,log_loss\nimport warnings\nimport time\nwarnings.filterwarnings('ignore')\nnp.random.seed(4950)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8eeff531588887d877378e6d93762469f68e4faf"
      },
      "cell_type": "code",
      "source": "import os\nos.listdir('../input/elosinglemodel0105/')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "337eacd2e6e13207d06cb90f9f8cd352d300b323"
      },
      "cell_type": "code",
      "source": "def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_train = pd.read_csv('../input/elo-merchant-category-recommendation/train.csv')\ndf_test = pd.read_csv('../input/elo-merchant-category-recommendation/test.csv')\ndf_hist_trans = pd.read_csv('../input/elo-merchant-category-recommendation/historical_transactions.csv')\ndf_new_merchant_trans = pd.read_csv('../input/elo-merchant-category-recommendation/new_merchant_transactions.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "252db1c4842288dc2c4c5a116daa5674073c6daf"
      },
      "cell_type": "code",
      "source": "df_hist_trans = reduce_mem_usage(df_hist_trans)\ndf_new_merchant_trans = reduce_mem_usage(df_new_merchant_trans)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "773ebf1438426f78b811e5294a990eb0220d7430"
      },
      "cell_type": "code",
      "source": "df_hist_trans.purchase_date=pd.to_datetime(df_hist_trans.purchase_date)\npur_date=df_hist_trans[df_hist_trans.month_lag==0].groupby('card_id').purchase_date.max()\ncard_id_nan_unique=df_hist_trans[df_hist_trans.card_id.isin(pur_date.index)==False].card_id.unique()\ndf=pd.DataFrame(card_id_nan_unique)\ndf['month_lag_date']=pd.to_datetime('2018-02') # Seetting all nan values to 2018 Feb\ndf.set_index(0,inplace=True)\nnew_map=df.month_lag_date\nmethod=pur_date.append(new_map)\ndel df,pur_date\ngc.collect()\ntime.sleep(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "75925be9c1dc04dbd30a9d4b1c75121d12ad51e7"
      },
      "cell_type": "code",
      "source": "df_train.dtypes",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a33509ad47e4ba94082f62a7b4caebdd1f32227b"
      },
      "cell_type": "code",
      "source": "df_train['outliers'] = 0\ndf_train.loc[df_train['target'] < -30, 'outliers'] = 1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "71f89a3b8a93b2f2feb2cd0a45f860cde33687be"
      },
      "cell_type": "code",
      "source": "for df in [df_hist_trans,df_new_merchant_trans]:\n#     df['category_2'].fillna(1.0,inplace=True)\n#     df['category_3'].fillna('A',inplace=True)\n#     df['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True)\n    #修改特征\n    df['category_2'].fillna(-1,inplace=True)\n    df['category_3'].fillna('other',inplace=True)\n    df['merchant_id'].fillna('other',inplace=True)\n#     df.loc[df['installments'].isin([999,-1]),'installments'] = 0 ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dda90662d05e22310dd713df106ea07f4b8bccfc"
      },
      "cell_type": "code",
      "source": "def get_new_columns(name,aggs):\n    #for for 写法 nice\n    return [name + '_' + k + '_' + agg for k in aggs.keys() for agg in aggs[k]]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "690ba01a38f524e9345b419200f588f937bc067a"
      },
      "cell_type": "code",
      "source": "for df in [df_hist_trans,df_new_merchant_trans]:\n    df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n    df['year'] = df['purchase_date'].dt.year\n    df['weekofyear'] = df['purchase_date'].dt.weekofyear\n    df['month'] = df['purchase_date'].dt.month\n    df['dayofweek'] = df['purchase_date'].dt.dayofweek\n    df['weekend'] = (df.purchase_date.dt.weekday >=5).astype(int)\n    df['hour'] = df['purchase_date'].dt.hour\n    df['authorized_flag'] = df['authorized_flag'].map({'Y':1, 'N':0})\n    df['category_1'] = df['category_1'].map({'Y':1, 'N':0}) \n    #https://www.kaggle.com/c/elo-merchant-category-recommendation/discussion/73244\n    df['month_diff'] = ((datetime.datetime(2018,6,1) - df['purchase_date']).dt.days)//30\n    df['month_diff'] += df['month_lag']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "80aadb70cf47be8cf45b681d73214ace43fe8d1d"
      },
      "cell_type": "code",
      "source": "#因为根据auth_flag 将特征分成了两部分，这里聚合一个全局的auth_flag 特征\naggs = {}\naggs['purchase_amount'] = ['sum']\n# aggs['installments'] = ['sum','max','min','mean','var','median']\naggs['authorized_flag'] = ['sum', 'mean','std']\naggs['card_id'] = ['size']\nauth_flag = df_hist_trans.groupby(['card_id']).agg(aggs)\nauth_flag.columns = get_new_columns('auth_flag',aggs)\nauth_flag.reset_index(inplace=True)\ndf_train = df_train.merge(auth_flag,on='card_id',how='left')\ndf_test = df_test.merge(auth_flag,on='card_id',how='left')\ndel auth_flag\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "49db45166453de5e0509d5e43d52604344954744"
      },
      "cell_type": "code",
      "source": "#对 authorized_flag进行结果编码\naggs = {}\nfor col in ['category_2','category_3','state_id','subsector_id','merchant_category_id','city_id']:\n    df_hist_trans[col+'_auth_mean'] = df_hist_trans.groupby([col])['authorized_flag'].transform('mean')\n    df_hist_trans[col+'_auth_sum'] = df_hist_trans.groupby([col])['authorized_flag'].transform('sum') \n    aggs[col+'_auth_mean'] = ['mean']\n    aggs[col+'_auth_sum'] = ['sum'] \nauth_encoder = df_hist_trans.groupby(['card_id']).agg(aggs)\nauth_encoder.columns = get_new_columns('auth_encoder',aggs)\nauth_encoder.reset_index(inplace=True)\ndf_train = df_train.merge(auth_encoder,on='card_id',how='left')\ndf_test = df_test.merge(auth_encoder,on='card_id',how='left')\ndel auth_encoder\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5b08ce613977fc59acfdac57c208ba71db6a5ea3"
      },
      "cell_type": "code",
      "source": "def aggregate_per_month(prefix,history,agg_func):\n    grouped = history.groupby(['card_id', 'month_lag'])\n    intermediate_group = grouped.agg(agg_func)\n    intermediate_group.columns = [prefix + '_'.join(col).strip() for col in intermediate_group.columns.values]\n    intermediate_group.reset_index(inplace=True)\n\n    final_group = intermediate_group.groupby('card_id').agg(['mean', 'std'])\n    final_group.columns = [prefix + '_'.join(col).strip() for col in final_group.columns.values]\n    final_group.reset_index(inplace=True) \n    return final_group",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "508c91c2f30371a3bb5a8e5daac36f43749c55c7"
      },
      "cell_type": "code",
      "source": "#对授权码 进行按月聚合\nagg_func = {'authorized_flag': [ 'sum', 'mean','median']}\nfinal_group =  aggregate_per_month('agg_per_month_total',df_hist_trans,agg_func) \ndf_train = df_train.merge(final_group,on='card_id',how='left')\ndf_test = df_test.merge(final_group,on='card_id',how='left')\ndel final_group",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6dfd525f8227507cad2d8d3a6144bb1708f7e462"
      },
      "cell_type": "code",
      "source": "authorized_transactions = df_hist_trans[df_hist_trans['authorized_flag'] == 1]\ndf_hist_trans = df_hist_trans[df_hist_trans['authorized_flag'] == 0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "22f8b5af830b7591197956f40b8b575d1e75c713"
      },
      "cell_type": "code",
      "source": "df_hist_trans.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "53204f1e1752f4e51d96e7c05360898ababc655d"
      },
      "cell_type": "code",
      "source": "agg_func = {'purchase_amount': [ 'sum', 'mean', 'min', 'max', 'std']}\nfinal_group =  aggregate_per_month('agg_per_month_auth',authorized_transactions,agg_func) \ndf_train = df_train.merge(final_group,on='card_id',how='left')\ndf_test = df_test.merge(final_group,on='card_id',how='left')\ndel final_group\nfinal_group =  aggregate_per_month('agg_per_month_hist',df_hist_trans,agg_func) \ndf_train = df_train.merge(final_group,on='card_id',how='left')\ndf_test = df_test.merge(final_group,on='card_id',how='left')\ndel final_group\nfinal_group =  aggregate_per_month('agg_per_month_hist_new',df_new_merchant_trans,agg_func) \ndf_train = df_train.merge(final_group,on='card_id',how='left')\ndf_test = df_test.merge(final_group,on='card_id',how='left')\ndel final_group\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ddf1d5bb0ade2b22b0f072c208c1506ea64503ea"
      },
      "cell_type": "code",
      "source": "i = 0\nfor df in [authorized_transactions,df_hist_trans]:\n    aggs = {}\n    for col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id',\n                'state_id','city_id']:\n        aggs[col] = ['nunique']\n\n    aggs['purchase_amount'] = ['sum','max','min','mean','var','median']\n    aggs['installments'] = ['sum','max','min','mean','var','median']\n    aggs['purchase_date'] = ['max','min']\n    aggs['month_lag'] = ['max','min','mean','var','median']\n    aggs['month_diff'] = ['mean','median']\n#     aggs['authorized_flag'] = ['sum', 'mean','median']\n    aggs['weekend'] = ['sum', 'mean']\n    aggs['category_1'] = ['sum', 'mean']\n    aggs['card_id'] = ['size']\n    #产生交叉特征，内存有问题\n    features = ['category_2','category_3','state_id','subsector_id','merchant_category_id','city_id'\n               ,'merchant_id']\n#     for coli in ['category_2','category_3','state_id','subsector_id','merchant_category_id','city_id']:\n#         for colj in ['category_2','category_3','state_id','subsector_id','merchant_category_id','city_id']:\n#             df[coli + colj] = df[coli].astype(str) + df[colj].astype(str)\n#             features.append(coli + colj)\n    for col in features:\n        df[col+'_mean'] = df.groupby([col])['purchase_amount'].transform('mean')\n        df[col+'_sum'] = df.groupby([col])['purchase_amount'].transform('sum') \n        aggs[col+'_mean'] = ['mean']\n        aggs[col+'_sum'] = ['sum'] \n    if i == 0:\n        prefix = 'auth_hist'\n    else:\n        prefix = 'hist'\n    new_columns = get_new_columns(prefix,aggs)\n    i += 1\n    # df_hist_trans.sort_values(['card_id','purchase_date'],inplace = True)\n\n    df_hist_trans_group = df.groupby('card_id').agg(aggs)\n    df_hist_trans_group.columns = new_columns\n    df_hist_trans_group.reset_index(drop=False,inplace=True)\n    df_hist_trans_group[prefix + '_purchase_date_diff'] = (df_hist_trans_group[prefix + '_purchase_date_max'] - df_hist_trans_group[prefix + '_purchase_date_min']).dt.days\n    df_hist_trans_group[prefix + '_purchase_date_average'] = df_hist_trans_group[prefix + '_purchase_date_diff']/df_hist_trans_group[prefix + '_card_id_size']\n    #df_hist_trans_group[prefix + '_purchase_date_uptonow'] = (df_hist_trans_group['card_id'].map(method) - df_hist_trans_group[prefix + '_purchase_date_max']).dt.days\n   \n    df_hist_trans_group[prefix + '_purchase_date_uptonow'] = (df_hist_trans_group['card_id'].map(method) - df_hist_trans_group[prefix + '_purchase_date_min']).dt.days\n    \n    ### This is old feature but I change the name to [prefix + '_first_purchase'] from [prefix + '_purchase_date_uptonow']\n    ### since it makes more sense. Number of days for the last purchase from month_lag_0\n\n    df_hist_trans_group[prefix + '_first_purchase'] = (df_hist_trans_group['card_id'].map(method) - df_hist_trans_group[prefix + '_purchase_date_max']).dt.days   \n#下面这个特征考虑了：有的人可能购买频率较低，但是还是忠实粉丝的情况\n#     df_hist_trans_group[prefix + '_purchase_date_uptonow_ave'] =  df_hist_trans_group[prefix + '_purchase_date_uptonow']/df_hist_trans_group[prefix + '_purchase_date_average']\n\n    #每一个card中未授权消费次数\n#     df_hist_trans_group[prefix + '_unauthorized_number'] = df_hist_trans_group[prefix + '_card_id_size'] - df_hist_trans_group[prefix + '_authorized_flag_sum']\n    #最近活跃时间，确实是一个强特征，感觉可以再挖出来几个特征，比如最近5次消费时间，最近10次消费时间，如果值比较小，说明最近很活跃\n    #没有效果\n    # grouped =  df_hist_trans.groupby('card_id')['purchase_date']\n    # df_hist_trans_group['hist_purchase_5thdate_uptonow'] =  (datetime.datetime.today() -grouped.shift(5)).dt.days\n    # df_hist_trans_group['hist_purchase_3thdate_uptonow'] =  (datetime.datetime.today() -grouped.shift(3)).dt.days\n    # df_hist_trans_group['hist_purchase_10thdate_uptonow'] =  (datetime.datetime.today() -grouped.shift(10)).dt.days\n    df_train = df_train.merge(df_hist_trans_group,on='card_id',how='left')\n    df_test = df_test.merge(df_hist_trans_group,on='card_id',how='left')\n    del df_hist_trans_group\n    time.sleep(5)\n    gc.collect()\n    del df;gc.collect()\n    time.sleep(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f7f5625db40db4395374991124fb796c9decd60b"
      },
      "cell_type": "code",
      "source": "aggs = {}\n#添加特征\nfor col in ['month','hour','weekofyear','dayofweek','year','subsector_id','merchant_id','merchant_category_id',\n            'state_id','city_id']:\n    aggs[col] = ['nunique']\n    \naggs['purchase_amount'] = ['sum','max','min','mean','var','median']\naggs['installments'] = ['sum','max','min','mean','var','median']\naggs['purchase_date'] = ['max','min']\naggs['month_lag'] = ['max','min','mean','var','median']\naggs['month_diff'] = ['mean','median']\naggs['weekend'] = ['sum', 'mean']\naggs['authorized_flag'] = ['sum', 'mean','median']\naggs['category_1'] = ['sum', 'mean']\naggs['card_id'] = ['size']\n\n#添加特征   \nfeatures = ['category_2','category_3','state_id','subsector_id','merchant_category_id','city_id'\n               ,'merchant_id']\n#产生交叉特征，内存有问题\n# for coli in ['category_2','category_3','state_id','subsector_id','merchant_category_id','city_id']:\n#     for colj in ['category_2','category_3','state_id','subsector_id','merchant_category_id','city_id']:\n#         df_new_merchant_trans[coli + colj] = df_new_merchant_trans[coli].astype(str) + df_new_merchant_trans[colj].astype(str)\n#         features.append(coli + colj)\nfor col in features:\n    df_new_merchant_trans[col+'_mean'] = df_new_merchant_trans.groupby([col])['purchase_amount'].transform('mean')\n    df_new_merchant_trans[col+'_sum'] = df_new_merchant_trans.groupby([col])['purchase_amount'].transform('sum')\n    aggs[col+'_mean'] = ['mean']\n    aggs[col+'_sum'] = ['sum']\nnew_columns = get_new_columns('new_hist',aggs)\n# df_new_merchant_trans.sort_values(['card_id','purchase_date'],inplace = True)\ndf_hist_trans_group = df_new_merchant_trans.groupby('card_id').agg(aggs)\ndf_hist_trans_group.columns = new_columns\ndf_hist_trans_group.reset_index(drop=False,inplace=True)\ndf_hist_trans_group['new_hist_purchase_date_diff'] = (df_hist_trans_group['new_hist_purchase_date_max'] - df_hist_trans_group['new_hist_purchase_date_min']).dt.days\ndf_hist_trans_group['new_hist_purchase_date_average'] = df_hist_trans_group['new_hist_purchase_date_diff']/df_hist_trans_group['new_hist_card_id_size']\n#df_hist_trans_group['new_hist_purchase_date_uptonow'] = (df_hist_trans_group['card_id'].map(method) - df_hist_trans_group['new_hist_purchase_date_max']).dt.days\ndf_hist_trans_group['new_hist_purchase_date_uptonow'] = (df_hist_trans_group['card_id'].map(method) - df_hist_trans_group['new_hist_purchase_date_max']).dt.days\n\n# new feature for the first purchase.This means total number of days from month_lag_0 to first purchase.\ndf_hist_trans_group['new_hist_first_purchase'] = (df_hist_trans_group['card_id'].map(method) - df_hist_trans_group['new_hist_purchase_date_min']).dt.days\n#下面这个特征考虑了：有的人可能购买频率较低，但是还是忠实粉丝的情况\n# df_hist_trans_group['new_hist_purchase_date_uptonow_ave'] =  df_hist_trans_group['new_hist_purchase_date_uptonow']/df_hist_trans_group['new_hist_purchase_date_average']\n\n# grouped = df_new_merchant_trans.groupby('card_id')['purchase_date']\n# df_hist_trans_group['new_hist_purchase_5thdate_uptonow'] =  (datetime.datetime.today() - grouped.shift(5)).dt.days\n# df_hist_trans_group['new_hist_purchase_3thdate_uptonow'] =  (datetime.datetime.today() - grouped.shift(3)).dt.days\n# df_hist_trans_group['new_hist_purchase_10thdate_uptonow'] =  (datetime.datetime.today() - grouped.shift(10)).dt.days\n\ndf_train = df_train.merge(df_hist_trans_group,on='card_id',how='left')\ndf_test = df_test.merge(df_hist_trans_group,on='card_id',how='left')\ndel df_hist_trans_group\ngc.collect()\ndel df_new_merchant_trans\ngc.collect()\ntime.sleep(5)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ce2082fc1fb0e3f8f7d27fc166aa7a8351b65504"
      },
      "cell_type": "code",
      "source": "for df in [df_train,df_test]:\n    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n    df['dayofweek'] = df['first_active_month'].dt.dayofweek\n    df['weekofyear'] = df['first_active_month'].dt.weekofyear\n    df['month'] = df['first_active_month'].dt.month\n    df['elapsed_time'] = (df['card_id'].map(method) - df['first_active_month']).dt.days\n    df['hist_first_buy'] = (df['hist_purchase_date_min'] - df['first_active_month']).dt.days\n    df['new_hist_first_buy'] = (df['new_hist_purchase_date_min'] - df['first_active_month']).dt.days\n    #添加特征\n    df['auth_hist_first_buy'] = (df['auth_hist_purchase_date_min'] - df['first_active_month']).dt.days\n    #修改特征\n    for f in ['hist_purchase_date_max','hist_purchase_date_min','new_hist_purchase_date_max',\\\n                     'new_hist_purchase_date_min','auth_hist_purchase_date_max','auth_hist_purchase_date_min']:\n        df[f] = df[f].astype(np.int64) * 1e-9\n    for f in ['auth_encoder_category_2_auth_sum_sum','auth_encoder_category_3_auth_sum_sum',\n            'auth_encoder_state_id_auth_sum_sum','auth_encoder_subsector_id_auth_sum_sum',\n            'auth_encoder_merchant_category_id_auth_sum_sum','auth_encoder_city_id_auth_sum_sum']:\n        df[f] = df[f].astype(np.int64) * 1e-9\n    #上面auth_flag已经聚合过了card_id_size ,purchase_amount\n#     df['card_id_total'] = df['new_hist_card_id_size']+df['hist_card_id_size']  + df['auth_hist_card_id_size']\n#     df['purchase_amount_total'] = df['new_hist_purchase_amount_sum']+df['hist_purchase_amount_sum']+df['auth_hist_purchase_amount_sum']\n#添加特征\n#下面这个特征对于识别异常点有一定帮助，但是训练model_without_outliers的时候需要删除这几个特征。\nfor f in ['feature_1','feature_2','feature_3']:\n    order_label1 = df_train.groupby([f])['outliers'].mean()\n    df_train[f+'_outliers_mean'] = df_train[f].map(order_label1)\n    df_test[f+'_outliers_mean'] = df_test[f].map(order_label1)\n    \n    order_label2 = df_train.groupby([f])['outliers'].sum()\n    df_train[f+'_outliers_sum'] = df_train[f].map(order_label2)\n    df_test[f+'_outliers_sum'] = df_test[f].map(order_label2)\n    \n# get_dummies 似乎有一点点不良影响\n# df_train = pd.get_dummies(df_train,columns =['feature_1','feature_2'])\n# df_test = pd.get_dummies(df_test,columns =['feature_1','feature_2'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3877e4f1418facb4da080ba31ef8ebae1724e7b1"
      },
      "cell_type": "code",
      "source": "#首次购买的时间居然早于首次激活的时间，进行调整\ndf_train.loc[df_train['auth_hist_first_buy'] < 0, 'auth_hist_first_buy'] = -1\ndf_train.loc[df_train['hist_first_buy'] < 0, 'hist_first_buy'] = -1\ndf_train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6ae5cbc0e9d6b54ab1aa1af908f8a798285163aa"
      },
      "cell_type": "code",
      "source": "# df_train_outliers = pd.read_csv('../input/elooutlier/outliers_train_output.txt',names = ['score'])\n# df_test_outliers = pd.read_csv('../input/elooutlier/outliers_test_output.txt',names = ['score'])\n# df_train['outliers_score'] = df_train_outliers['score'].values\n# df_test['outliers_score'] = df_test_outliers['score'].values\n# df_train.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e60284c84626f7c544cb68fa35f8b098a025eb7e"
      },
      "cell_type": "code",
      "source": "df_test.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c4f20f27679889542acfd60d1f1ac381b201ac43"
      },
      "cell_type": "code",
      "source": "exclude_features = []\nexclude_features += ['card_id', 'first_active_month','target','outliers']\ndf_train_columns = [c for c in df_train.columns if c not in exclude_features ]\ncategorical_feats = [c for c in df_train_columns if 'feature_' in c]\ntarget = df_train['target']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7125ca006be962f642dc1fbaae06d75dcacf405f"
      },
      "cell_type": "code",
      "source": "print(categorical_feats)\ndf_train.columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3f884f12c5bb4f57a227ad256c9f2793e2fae4ee"
      },
      "cell_type": "code",
      "source": "#训练分类模型\nparam = {'num_leaves': 31,\n         'min_data_in_leaf': 150, \n         'objective':'binary',\n         'max_depth': 6,\n         'learning_rate': 0.01,\n         \"boosting\": \"rf\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 11,\n         \"metric\": 'binary_logloss',\n         'scale_pos_weight':15,\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1,\n         \"random_state\": 2333}\nfolds = StratifiedKFold(n_splits=10, shuffle=True, random_state=4950)\noof = np.zeros(len(df_train))\npredictions = np.zeros(len(df_test))\nfeature_importance_df = pd.DataFrame()\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(df_train,df_train['outliers'].values)):\n    print(\"fold {}\".format(fold_))\n    trn_data = lgb.Dataset(df_train.iloc[trn_idx][df_train_columns], label=df_train['outliers'].iloc[trn_idx], categorical_feature=categorical_feats)\n    val_data = lgb.Dataset(df_train.iloc[val_idx][df_train_columns], label=df_train['outliers'].iloc[val_idx], categorical_feature=categorical_feats)\n\n    num_round = 10000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 600)\n    oof[val_idx] = clf.predict(df_train.iloc[val_idx][df_train_columns], num_iteration=clf.best_iteration)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = df_train_columns\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions += clf.predict(df_test[df_train_columns], num_iteration=clf.best_iteration) / folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(log_loss(df_train['outliers'], oof)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2e30bae195e12f26c2e1d440d6ee17111ce54709"
      },
      "cell_type": "code",
      "source": "# 计算结果，对于训练集，一个比较合适的召回率，似乎需要12w 训练样本，对于测试集，我们选择6w作为潜在的异常点吧。\n# 实验结果提升较小，可能测试集中异常点比例较低\ndf_outlier_prob = pd.DataFrame({\"card_id\":df_train[\"card_id\"].values})\ndf_outlier_prob[\"target\"] = oof\noutlier_id = pd.DataFrame(df_outlier_prob.sort_values(by='target',ascending = False).head(10000)['card_id'])\nfor k in range(10000,200000,10000):\n    outlier_id = pd.DataFrame(df_outlier_prob.sort_values(by='target',ascending = False).head(k)['card_id'])\n    out = set(outlier_id['card_id'].values)\n    i = 0\n    for id in df_train[df_train['outliers'] == 1]['card_id'].values:\n        if id in out:\n            i += 1\n    print(i,k)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "687c48c67d10e02c58218703bedfab098a7ec3de"
      },
      "cell_type": "code",
      "source": "2207 / 201917",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c6600a5e9d3ee51697ffa48ec5f76192cab060da"
      },
      "cell_type": "code",
      "source": "### 'target' is the probability of whether an observation is an outlier\ndf_outlier_prob = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\ndf_outlier_prob[\"target\"] = predictions\n# In case missing some predictable outlier, we choose top 25000 with highest outliers likelyhood.\noutlier_id = pd.DataFrame(df_outlier_prob.sort_values(by='target',ascending = False).head(18000)['card_id'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "70de726db3e1a4eea59c5de3abbbf17a4379a287"
      },
      "cell_type": "code",
      "source": "df_outlier_prob.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "abf268de0a9e534f04067ff00c2519034d95c621"
      },
      "cell_type": "markdown",
      "source": "# Seperate the train and test data into three part ,"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3126881e6065ec76bbf692c14028a9853d58f121"
      },
      "cell_type": "code",
      "source": "ratio = len(df_test) / len(df_train)\nratio",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "735313127748821537efe7eed0c974279baf40af"
      },
      "cell_type": "code",
      "source": "### 'target' is the probability of whether an observation is an outlier\ndf_outlier_prob = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\ndf_outlier_prob[\"target\"] = predictions\n# In case missing some predictable outlier, we choose top 25000 with highest outliers likelyhood.\noutlier_id = df_outlier_prob.sort_values(by='target',ascending = False).index\ntest_id_part = []\n# ratio = len(df_test) / len(df_train)\n# 参考李胃疼的思路，这里直接预测前18000个\ntest_id_part.append(outlier_id[:18000])\n# test_id_part.append(outlier_id[int(10000 * ratio) : int(50000 * ratio)])\ntest_id_part.append(outlier_id[18000:])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5851c09be86e6fe6d504600dc8af9fe469576d3b"
      },
      "cell_type": "code",
      "source": "df_outlier_prob = pd.DataFrame({\"card_id\":df_train[\"card_id\"].values})\ndf_outlier_prob[\"target\"] = oof\n# In case missing some predictable outlier, we choose top 25000 with highest outliers likelyhood.\noutlier_id = df_outlier_prob.sort_values(by='target',ascending = False).index\ntrain_id_part = []\ntrain_id_part.append(outlier_id[:50000])\n# train_id_part.append(outlier_id[10000:50000])\n# train_id_part.append(outlier_id[20000:])\nwithout_outliers = df_train[df_train['target'] > -33].index\ntrain_id_part.append(without_outliers)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5a28f02cb912edfa71310be65068414180ea40a7"
      },
      "cell_type": "code",
      "source": "c = [ c for c in df_train.loc[train_id_part[1]]['target'].values if c < -33]\nlen(c)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d632f725d8f0e023e45fd3f972f615cabce587b"
      },
      "cell_type": "code",
      "source": "targets = []\ntargets.append(df_train.loc[train_id_part[0]]['target'])\ntargets.append(df_train.loc[train_id_part[1]]['target'])\n# targets.append(df_train.loc[train_id_part[2]]['target'])\n\n#删除异常值编码，下面是没有异常值的训练集\n#下面这个特征对于识别异常点有一定帮助，但是训练model_without_outliers的时候需要删除这几个特征。\n# for f in ['feature_1','feature_2','feature_3']:\n#     del df_train[f + '_outliers_mean']\n#     del df_train[f + '_outliers_sum']\n#     del df_test[f + '_outliers_mean']\n#     del df_test[f + '_outliers_sum']\n# del df_train['outliers_score']\n# del df_test['outliers_score']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9455056bcb2f041f0ef8b69168df07bb34cf55ff"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a6518795ff8f08d9066b336b06c32d8a24db59b0"
      },
      "cell_type": "code",
      "source": "exclude_features = []\nexclude_features += ['card_id', 'first_active_month','target','outliers']\ndf_train_columns = [c for c in df_train.columns if c not in exclude_features ]\ncategorical_feats = [c for c in df_train_columns if 'feature_' in c]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b0be03770f27bda6de61fa9265871c3860221da0"
      },
      "cell_type": "code",
      "source": "param = {'num_leaves': 31,\n         'min_data_in_leaf': 32, \n         'objective':'regression',\n         'max_depth': -1,\n         'learning_rate': 0.01,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 42,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.1,\n         \"verbosity\": -1,\n         \"nthread\": 4,\n         \"random_state\": 4950}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c9bbc95244978b519d94131907b547c2b6c94191"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import KFold\n# all_train = np.zeros(len(df_train))\nall_test = np.zeros(len(df_test))\nfor i in range(len(test_id_part)):\n    folds = KFold(n_splits=10, shuffle=True, random_state=4950)\n    train = df_train.loc[train_id_part[i]]\n    test = df_test.loc[test_id_part[i]]\n    oof = np.zeros(len(train))\n    predictions = np.zeros(len(test))\n    target = targets[i]\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train,target.values)):\n        print(\"fold {}\".format(fold_))\n        trn_data = lgb.Dataset(train.iloc[trn_idx][df_train_columns], label=target.iloc[trn_idx], categorical_feature=categorical_feats)\n        val_data = lgb.Dataset(train.iloc[val_idx][df_train_columns], label=target.iloc[val_idx], categorical_feature=categorical_feats)\n#         num_round = 200\n        num_round = 10000\n        clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=100, early_stopping_rounds = 600)\n        oof[val_idx] = clf.predict(train.iloc[val_idx][df_train_columns], num_iteration=clf.best_iteration)\n        predictions += clf.predict(test[df_train_columns], num_iteration=clf.best_iteration) / folds.n_splits\n\n    print(np.sqrt(mean_squared_error(oof, target)))\n#     all_train[train_id_part[i]] = oof\n    all_test[test_id_part[i]] = predictions\n# np.sqrt(mean_squared_error(all_train, df_train['target']))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "40b64481054fa71e692829c7039eccceb31b77fe"
      },
      "cell_type": "code",
      "source": "multi_model = pd.DataFrame({\"card_id\":df_test[\"card_id\"].values})\nmulti_model[\"target\"] = all_test",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "226480cd7ef8508ce2638be04556a6a8e7062a04"
      },
      "cell_type": "code",
      "source": "multi_model[80000:80003]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6ca3a2e7f11436472e40a36860ec0fd6136e21c3"
      },
      "cell_type": "code",
      "source": "multi_model.to_csv('wang_lgb_cv_with_multi_model.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "53acff9a7d6a0d8c4aba97491940374252829af7"
      },
      "cell_type": "code",
      "source": "best_submission = pd.read_csv('../input/elosinglemodel0105/submission.csv')\nbest_submission[80000:80003]",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}